{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SofiaSchweitzer/crc_summer_school/blob/main/generative_diffusion_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbEyS62D9Drn"
      },
      "source": [
        "## Generating LHC data with Diffusion Models\n",
        "\n",
        "#### Background\n",
        "\n",
        "Simulations for LHC physics is a major computational task at the experiment, and will become more computationally intensive in the coming years, so if ML techniques can speed up some aspects of this then it would be very useful. The process we are studying is Drell-Yan: $pp\\rightarrow Z\\rightarrow \\mu\\mu$\n",
        "\n",
        "#### Interesting papers\n",
        "\n",
        "- Jet Diffusion versus JetGPT â€” Modern Networks for the LHC<br>\n",
        "  *Anja Butter, Nathan Huetsch, Sofia Palacios Schweitzer, Tilman Plehn, Peter Sorrenson, and Jonas Spinner*<br>\n",
        "  https://arxiv.org/pdf/2305.10475.pdf\n",
        "- Denoising Diffusion Probabilistic Models<br>\n",
        "  *Jonathan Ho, Ajay Jain, Pieter Abbeel*<br>\n",
        "  https://arxiv.org/pdf/2006.11239.pdf\n",
        "- Flow Matching for Generative Modeling<br>\n",
        "  *Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matt Le*<br>\n",
        "  https://arxiv.org/pdf/2210.02747.pdf\n",
        "\n",
        "#### Outline\n",
        "\n",
        "- Imports\n",
        "- Loading the data\n",
        "- Study the data\n",
        "- Preprocessing\n",
        "- Defining the diffusion model\n",
        "- Training the model\n",
        "- Study the results\n",
        "\n",
        "with a CFM network."
      ],
      "id": "LbEyS62D9Drn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRMp1f239Dro"
      },
      "source": [
        "### Imports"
      ],
      "id": "IRMp1f239Dro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6BWD4o49Dro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "id": "p6BWD4o49Dro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu2E9do-9Drp"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "id": "Vu2E9do-9Drp"
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "_kxVukZTMfe8"
      },
      "id": "_kxVukZTMfe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETUfDbzc9Drp"
      },
      "source": [
        "### Loading the data"
      ],
      "id": "ETUfDbzc9Drp"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fi/gvmelw7u619moo8nyg3j7/ErUMData.zip?e=1&file_subpath=%2Fdy_trn_data.npy&rlkey=kq4do1fmalppjt2v24lzau4li&st=3umjk6s1&dl=0\"\n",
        "!unzip 'ErUMData.zip?e=1&file_subpath=%2Fdy_trn_data.npy&rlkey=kq4do1fmalppjt2v24lzau4li&st=3umjk6s1&dl=0'"
      ],
      "metadata": {
        "id": "nrPk8nhcNj9e"
      },
      "id": "nrPk8nhcNj9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBWgt49g9Drq"
      },
      "outputs": [],
      "source": [
        "train_data = np.load(\"dy_trn_data.npy\")\n",
        "val_data = np.load(\"dy_val_data.npy\")\n",
        "test_data = np.load(\"dy_tst_data.npy\")"
      ],
      "id": "TBWgt49g9Drq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjRmpfin9Drq"
      },
      "outputs": [],
      "source": [
        "train_data.shape, val_data.shape, test_data.shape"
      ],
      "id": "xjRmpfin9Drq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403_MoJe9Drq"
      },
      "source": [
        "Each element of the data has 8 entries, corresponding to the 4-momenta of each muon in the process.\n",
        "\n",
        "Each entry has the form $[E_1, p_{x,1}, p_{y,1}, p_{z,1}, E_2, p_{x,2}, p_{y,2}, p_{z,2}]$."
      ],
      "id": "403_MoJe9Drq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Although studying the data is essential for ML application, the focus of this tutorial lays elsewhere. For now, it is sufficient to simply run the commands of the sections \"Study the data\" and \"Preprocessing\" and you can look at them after the tutorial in more detail."
      ],
      "metadata": {
        "id": "Y1_mqkod-KJO"
      },
      "id": "Y1_mqkod-KJO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLet1pLv9Drr"
      },
      "source": [
        "### Study the data"
      ],
      "id": "FLet1pLv9Drr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cir0VejG9Drr"
      },
      "source": [
        "We need to be able to calculate the invariant mass of each event, and the $p_T$ of the event and the jets.\n",
        "\n",
        "Let's write a function for this:"
      ],
      "id": "cir0VejG9Drr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSK_kzb29Drs"
      },
      "outputs": [],
      "source": [
        "def get_obs(events):\n",
        "    muon1_4m = events[...,0:4]\n",
        "    muon2_4m = events[...,4:]\n",
        "    event_4m = muon1_4m + muon2_4m\n",
        "\n",
        "    event_im = np.sqrt(event_4m[...,0]**2 - event_4m[...,1]**2 - event_4m[...,2]**2 - event_4m[...,3]**2)\n",
        "\n",
        "    event_pt = np.sqrt(event_4m[...,1]**2 + event_4m[...,2]**2)\n",
        "    muon1_pt = np.sqrt(muon1_4m[...,1]**2 + muon1_4m[...,2]**2)\n",
        "    muon2_pt = np.sqrt(muon2_4m[...,1]**2 + muon2_4m[...,2]**2)\n",
        "\n",
        "    return event_im, event_pt, muon1_pt, muon2_pt"
      ],
      "id": "WSK_kzb29Drs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HehQnFSq9Drs"
      },
      "source": [
        "Now test it for the first two events in the training data:"
      ],
      "id": "HehQnFSq9Drs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_zXg9SM9Drs"
      },
      "outputs": [],
      "source": [
        "get_obs(train_data[0:2])"
      ],
      "id": "K_zXg9SM9Drs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQYN2Oay9Drs"
      },
      "source": [
        "The $p_T$ of the events will always be zero here due to the fact that the initial incoming protons have no transverse momentum. For the same reason, the $p_T$ of the first and second muon are equal.\n",
        "\n",
        "Let's get the observables for the whole test dataset:"
      ],
      "id": "ZQYN2Oay9Drs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9rpPx1r9Drs"
      },
      "outputs": [],
      "source": [
        "test_event_ims, test_event_pts, test_muon1_pts, test_muon2_pts = get_obs(test_data)"
      ],
      "id": "z9rpPx1r9Drs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c4xC2T69Drt"
      },
      "source": [
        "Let's plot the invariant mass of the events:"
      ],
      "id": "4c4xC2T69Drt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQZJXsBn9Drt"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "axs.hist(test_event_ims, density=True, bins=50, histtype=\"step\")\n",
        "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "plt.show()"
      ],
      "id": "bQZJXsBn9Drt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DypgMsh9Drt"
      },
      "source": [
        "Here we see the clear $Z$ mass peak at $\\simeq 90$ GeV. Now let's look at $p_T$ distributions:"
      ],
      "id": "4DypgMsh9Drt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM7hCsB39Drt"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(0, 70, 50)\n",
        "axs.hist(test_muon1_pts, density=True, bins=bins, histtype=\"step\", label=r\"$\\mu_1$\")\n",
        "axs.hist(test_muon2_pts, density=True, bins=bins, histtype=\"step\", label=r\"$\\mu_2$\")\n",
        "\n",
        "axs.set_xlabel(\"$p_{T}$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "gM7hCsB39Drt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RybU485b9Drt"
      },
      "source": [
        "These distributions are completely overlapping with a peak near $m_Z/2$.\n",
        "\n",
        "In the collider the incoming particles have no momentum in the $x$ or $y$ directions, therefore momentum conservation means that the final state should also not have any momentum in these directions.  This means that we have redundant degrees of freedom that can be removed from the data.\n",
        "\n",
        "We can check this:"
      ],
      "id": "RybU485b9Drt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p79qzuBr9Drt"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.arange(-5, 5, step=0.1)\n",
        "axs.hist(test_data[:,1]+test_data[:,5], density=True, bins=bins, histtype=\"step\", label=\"$p_{x,1}+p_{x_2}$\")\n",
        "axs.hist(test_data[:,2]+test_data[:,6], density=True, bins=bins, histtype=\"step\", label=\"$p_{y,1}+p_{y_2}$\")\n",
        "\n",
        "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "p79qzuBr9Drt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNKELuTE9Dru"
      },
      "source": [
        "Muons have a mass of about $106$ MeV.  This is much smaller than the typical energy scales involved in the Drell-Yan process we are studying, so we can assume to a good approximation that the mass of the final state particles are zero.  These final state particles are on-shell, i.e. $p^2=m^2=0$, so we can also use this as a constraint.  We can check that this approximately holds:"
      ],
      "id": "rNKELuTE9Dru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxiD7hio9Dru"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.arange(-5, 5, step=0.1)\n",
        "axs.hist(test_data[:,0]**2-test_data[:,1]**2-test_data[:,2]**2-test_data[:,3]**2,\n",
        "         density=True, bins=bins, label=\"$m_1^2$\", histtype=\"step\")\n",
        "axs.hist(test_data[:,4]**2-test_data[:,5]**2-test_data[:,6]**2-test_data[:,7]**2,\n",
        "         density=True, bins=bins, label=\"$m_2^2$\", histtype=\"step\")\n",
        "\n",
        "axs.set_xlabel(\"$m^2$ (GeV$^2$)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "IxiD7hio9Dru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUAYGX4w9Dru"
      },
      "source": [
        "We can remove 4 degrees of freedom from the data. For the remaining 4 degrees of freedom, we can choose a more suitable representation that takes into account the symmetries of our data:\n",
        "- $p_T = \\sqrt{p_{x,1}^2+p_{y,1}^2}$\n",
        "- $\\eta_1 = \\text{arctanh}\\left(\\frac{p_{z,1}}{\\sqrt{p_{x,1}^2+p_{y,1}^2+p_{z,1}^2}}\\right)$\n",
        "- $\\eta_2 = \\text{arctanh}\\left(\\frac{p_{z,2}}{\\sqrt{p_{x,2}^2+p_{y,2}^2+p_{z,2}^2}}\\right)$\n",
        "- $\\phi = \\text{arctan2}(p_{y,1}, p_{x,1})$"
      ],
      "id": "gUAYGX4w9Dru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bncNrkuV9Dru"
      },
      "source": [
        "### Preprocessing"
      ],
      "id": "bncNrkuV9Dru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu2vZCKO9Dru"
      },
      "source": [
        "For data to work well with generative models operating on a Gaussian latent space (a typical choice for diffusion models or normalizing flows), we should avoid sharp edges in the input data. We know that $\\phi$ follows a uniform distribution, so we can transform it into something closer to a Gaussian by applying $\\text{arctanh}$. We also apply a component-wise normalization step such that the data has a mean of $0$ and a standard deviation of $1$.\n",
        "\n",
        "Let's write some functions to apply the preprocessing to the training data and invert it for the generated data:"
      ],
      "id": "nu2vZCKO9Dru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyE6zGtg9Dru"
      },
      "outputs": [],
      "source": [
        "def apply_preprocessing(data_full):\n",
        "    pt = np.sqrt(data_full[...,1]**2 + data_full[...,2]**2)\n",
        "    eta1 = np.arctanh(data_full[...,3] / np.sqrt(data_full[...,1]**2 + data_full[...,2]**2 + data_full[...,3]**2))\n",
        "    eta2 = np.arctanh(data_full[...,7] / np.sqrt(data_full[...,5]**2 + data_full[...,6]**2 + data_full[...,7]**2))\n",
        "    phi1 = np.arctanh(np.arctan2(data_full[...,2], data_full[...,1]) / np.pi)\n",
        "    return np.stack((pt, eta1, eta2, phi1), axis=-1)\n",
        "\n",
        "def invert_preprocessing(data_red):\n",
        "    pt = data_red[...,0]\n",
        "    eta1 = data_red[...,1]\n",
        "    eta2 = data_red[...,2]\n",
        "    phi1 = np.arctan(data_red[...,3]) * np.pi\n",
        "    px1 = pt * np.cos(phi1)\n",
        "    py1 = pt * np.sin(phi1)\n",
        "    pz1 = pt * np.sinh(eta1)\n",
        "    e1 = np.sqrt(px1**2 + py1**2 + pz1**2)\n",
        "    px2 = -px1\n",
        "    py2 = -py1\n",
        "    pz2 = pt * np.sinh(eta2)\n",
        "    e2 = np.sqrt(px2**2 + py2**2 + pz2**2)\n",
        "    return np.stack((e1, px1, py1, pz1, e2, px2, py2, pz2), axis=-1)"
      ],
      "id": "fyE6zGtg9Dru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVo9ZQYL9Dru"
      },
      "source": [
        "Now we can apply them to the training data and create a torch Dataset and DataLoader:"
      ],
      "id": "uVo9ZQYL9Dru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4hSZfjj9Dru"
      },
      "outputs": [],
      "source": [
        "train_data_preproc = apply_preprocessing(train_data)\n",
        "train_mean = np.mean(train_data_preproc, axis=0)\n",
        "train_std = np.std(train_data_preproc, axis=0)\n",
        "train_data_normalized = torch.tensor((train_data_preproc - train_mean) / train_std, dtype=torch.float32)\n",
        "train_dataset = TensorDataset(train_data_normalized)\n",
        "train_dataloader = DataLoader(train_data_normalized, batch_size=10000, shuffle=True, generator=torch.Generator(device))"
      ],
      "id": "M4hSZfjj9Dru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUt8DsO9Dru"
      },
      "source": [
        "Let's take a look at the data the generative model has to learn:"
      ],
      "id": "NaUt8DsO9Dru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvD0FVp_9Drv"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.arange(-5, 5, step=0.1)\n",
        "for i in range(4):\n",
        "    axs.hist(\n",
        "        train_data_normalized[:, i].cpu().numpy(), density=True, bins=bins, label=f\"input {i}\", histtype=\"step\"\n",
        "    )\n",
        "\n",
        "axs.set_xlabel(\"training data\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "fvD0FVp_9Drv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUnjEreP9Drx"
      },
      "source": [
        "## Please fill out the blanks."
      ],
      "id": "EUnjEreP9Drx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1: Conditional Flow Matching\n",
        "\n",
        "In principle one can categorize Conditional Flow Matching (CFM) models as a specific kind of diffusion model. The idea here is to see the diffusion as a continous process. At time $t=0$ we are in our physical phase space and at time $t=1$ we are in our Gaussian latent space. The process is goverend by a velocity field\n",
        "\\begin{align}\n",
        "\\frac{dx}{dt} = v(x,t)\n",
        "\\end{align}\n",
        "and its inital condition.\n",
        "In principle we could learn this veloctiy field as a regression task using a MSE loss. However, how can we define it? We start by describing the diffusion process by a linear trajectory\n",
        "\\begin{align}\n",
        "x(t, x_0, x_1) = (1-t)x_0 + tx_1\n",
        "\\end{align}\n",
        "with $x_0 \\sim p(x_0)$ and $x_1 \\sim \\mathcal{N}(0,1)$ for at any time t.\n",
        "The corresponding veloctiy field yields\n",
        "\\begin{align}\n",
        "v(x,t, x_0, x_1) = x_1 - x_0.\n",
        "\\end{align}\n",
        "Why is this useful? We want to generate $x_0$, why are we interested in learning a quantitiy, which is only defined once $x_0$ is known? Well luckily, it can be shown that\n",
        "\\begin{align}\n",
        "\\mathrm{argmin}_\\theta \\left< (v_\\theta(x_t,t) - v(x,t))^2\\right>_{x,t \\sim p(x,t)} = \\mathrm{argmin}_\\theta \\left< (v_\\theta(x_t,t) - v(x,t,x_0, x_1))^2 \\right>_{x_0\\sim p(x_0), t\\sim U(0,1)},\n",
        "\\end{align}\n",
        "where $v_\\theta(x_t,t)$ is the network output. Hence, we have all ingredients to cook a tractable loss function. Once we are finished training we can simply solve\n",
        "\\begin{align}\n",
        "x_0 = x_1 - \\int_0^1 v_\\theta (x,t) d_t\n",
        "\\end{align}\n",
        "using a fast ODE-sampler.\n",
        "\n",
        "What is the network input? How many dimensions has the input?\n",
        "\n",
        "\n",
        "Implement the CFM model loss function using the following steps:\n",
        "1. Randomly draw the timestep $t$ from a uniform distribution.\n",
        "2. Sample Gaussian noise.\n",
        "3. Compute $x_t$ using the function above.\n",
        "4. Call the network as a function of $x_t$ and $t$.\n",
        "5. Compute the MSE loss."
      ],
      "metadata": {
        "id": "5YRCXRRp-vc2"
      },
      "id": "5YRCXRRp-vc2"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "id": "bkYg-VR8c6u8"
      },
      "id": "bkYg-VR8c6u8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use ode solver that can operate on gpu\n",
        "from torchdiffeq import odeint\n",
        "\n",
        "class CFM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dim: int,     # number of features in the data\n",
        "        hidden_dim: int,   # number of hidden layer nodes\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dim = data_dim\n",
        "\n",
        "        # TODO: Build network to predict the velocity field with\n",
        "        # 3 hidden layers with hidden_dim\n",
        "\n",
        "\n",
        "    def batch_loss(\n",
        "        self,\n",
        "        x: torch.Tensor, # input data, shape (n_batch, data_dim)\n",
        "    ) -> torch.Tensor:   # loss, shape (n_batch, )\n",
        "\n",
        "        # TODO: Implement the batch_loss\n",
        "\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        n_samples: int,        # number of samples\n",
        "    ) -> torch.Tensor:         # sampled data, shape (n_samples, data_dim) or (n_steps, n_samples, data_dim)\n",
        "        dtype=torch.float32\n",
        "        x_1 = torch.randn(n_samples, self.data_dim, device=device, dtype=dtype)\n",
        "\n",
        "        def net_wrapper(t, x_t):\n",
        "          t = t * torch.ones_like(x_t[:, [0]], dtype=dtype, device=device)\n",
        "          nn_out = self.net(torch.cat([t, x_t], dim=1))\n",
        "          return nn_out\n",
        "\n",
        "        x_t = odeint(net_wrapper,\n",
        "                     x_1,\n",
        "                     torch.tensor([1.,0.], dtype=dtype, device=device)\n",
        "                             )\n",
        "\n",
        "        return x_t[-1]"
      ],
      "metadata": {
        "id": "nxPuThyTZD14"
      },
      "id": "nxPuThyTZD14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat training and sampling with a CFM."
      ],
      "metadata": {
        "id": "NgllIumQw385"
      },
      "id": "NgllIumQw385"
    },
    {
      "cell_type": "code",
      "source": [
        "cfm = CFM(\n",
        "    data_dim = 4,\n",
        "    hidden_dim = 64,\n",
        ")\n",
        "epochs = 50\n",
        "\n",
        "optimizer = torch.optim.Adam(cfm.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    epoch_losses = []\n",
        "    for batch, x in enumerate(train_dataloader):\n",
        "        loss = cfm.batch_loss(x)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    print(f\"Epoch {epoch+1}: loss = {epoch_loss}\")\n",
        "    losses[epoch] = epoch_loss"
      ],
      "metadata": {
        "id": "LybEFG3hEAkd"
      },
      "id": "LybEFG3hEAkd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO plot epochs vs losses\n"
      ],
      "metadata": {
        "id": "JfC0xuxvEGE3"
      },
      "id": "JfC0xuxvEGE3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    sample = cfm.sample(10000).cpu().numpy()\n",
        "sample_pp = invert_preprocessing(sample * train_std + train_mean)\n",
        "gen_event_im, _, gen_pt, _ = get_obs(sample_pp)"
      ],
      "metadata": {
        "id": "Sb6a8hJXEmxY"
      },
      "id": "Sb6a8hJXEmxY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(0,200,50)\n",
        "axs.hist(sample_pp[:,0], bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_data[:,0], bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$E_1$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h4Gg9_btUlns"
      },
      "id": "h4Gg9_btUlns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(0, 70, 50)\n",
        "axs.hist(gen_pt, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_muon1_pts, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJqRtm_7VzZX"
      },
      "id": "YJqRtm_7VzZX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(60, 120, 50)\n",
        "axs.hist(gen_event_im, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_event_ims, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HSSJ3D26V0f6"
      },
      "id": "HSSJ3D26V0f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0bYDt7cXDAR"
      },
      "id": "f0bYDt7cXDAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus exercise"
      ],
      "metadata": {
        "id": "qY15ObzX_C0U"
      },
      "id": "qY15ObzX_C0U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUDB9Wms9Drv"
      },
      "source": [
        "### Defining the diffusion model\n",
        "A Denoising Diffusion Probabilistic Model (DDPM) is a diffusion model, which - in the forward direction - gradually adds noise to a given phase space point $x_0\\sim p(x_0)$ follwing a noise scheduler $\\beta_t$. After $T$ discrete time steps only Gaussian noise is left. For a given time $t$ we can show that under this formalism\n",
        "\\begin{align}\n",
        "x(t,x_0) = \\sqrt{1-\\bar{\\beta}_t} x_0 + \\sqrt{\\bar{\\beta}_t} \\epsilon\n",
        "\\end{align}\n",
        "holds, with $1- \\bar{\\beta}_t = \\prod_{i=0}^{t} (1-\\beta_i)$ and $\\epsilon\\sim\\mathcal{N}(0,1)$.\n",
        "The ML tasks is it starting from a Gaussian sample $x_T\\sim \\mathcal{N}(0,1)$ to gradually remove or *denoise* this sample until it resembels a point in our physical phase space $x_0 \\sim p(x_0)$. In practise we learn $\\epsilon_\\theta(x_t,t)$, the amount of noise added at time $t$ by a MSE loss. After training we can substract $\\epsilon_\\theta$ step-by-step. Note that in literatur often times $\\alpha_i := (1-\\beta_i)$ is introduced."
      ],
      "id": "eUDB9Wms9Drv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zy-uQ1l9Drv"
      },
      "source": [
        "#### Exercise 1: Diffusion loss function\n",
        "Implement the diffusion model loss function using the following steps:\n",
        "1. Randomly draw the timestep $t$ from a uniform distribution.\n",
        "2. Sample Gaussian noise.\n",
        "3. Compute $x_t$ using the function `compute_xt` as explained above\n",
        "4. Compute the prefactor of the loss using `compute_relative_factor` (this is to ensure that the loss is a likelihood loss)\n",
        "5. Call the network as a function of $x_t$ and $t$\n",
        "6. Compute the MSE loss with the correct prefactor."
      ],
      "id": "8zy-uQ1l9Drv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRRPVnih9Drv"
      },
      "outputs": [],
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dim: int,     # number of features in the data\n",
        "        n_steps: int,      # number of time steps\n",
        "        n_layers: int,     # number of network layers\n",
        "        hidden_dim: int,   # number of hidden layer nodes\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.data_dim = data_dim\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "        # Initialize alpha, beta and sigma constants for the given number of time steps\n",
        "        self.betas = self.linear_beta_schedule(n_steps)\n",
        "\n",
        "        alphas = 1 - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value=1.)\n",
        "        self.one_minus_alphas_bar = 1 - alphas_bar\n",
        "        self.sqrt_alphas = torch.sqrt(alphas)\n",
        "        self.sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
        "        self.sqrt_one_minus_alphas_bar = torch.sqrt(self.one_minus_alphas_bar)\n",
        "        self.sigmas = torch.sqrt(self.betas)\n",
        "\n",
        "        # Build network\n",
        "        layers = []\n",
        "        layer_dim_in = data_dim + 1\n",
        "        for i in range(n_layers - 1):\n",
        "            layers.append(nn.Linear(layer_dim_in, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layer_dim_in = hidden_dim\n",
        "        layers.append(nn.Linear(layer_dim_in, data_dim))\n",
        "        torch.nn.init.zeros_(layers[-1].weight)\n",
        "        torch.nn.init.zeros_(layers[-1].bias)\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def linear_beta_schedule(\n",
        "        self,\n",
        "        n_steps: int   # number of time steps\n",
        "    ) -> torch.Tensor: # beta values at the time steps, shape (n_steps, )\n",
        "        scale = 1000 / n_steps\n",
        "        beta_start = scale * 0.0001\n",
        "        beta_end = scale * 0.02\n",
        "        return torch.linspace(beta_start, beta_end, n_steps, dtype=torch.float64)\n",
        "\n",
        "    def compute_xt(\n",
        "        self,\n",
        "        x0: torch.Tensor,    # data point x_0, shape (n_batch, data_dim)\n",
        "        t: torch.Tensor,     # time step, shape (n_batch, 1)\n",
        "        noise: torch.Tensor, # gaussian noise, shape (n_batch, data_dim)\n",
        "    ) -> torch.Tensor:       # noisy point x_t, shape (n_batch, data_dim)\n",
        "        # TODO: Compute x_t as explained above\n",
        "\n",
        "\n",
        "    def compute_relative_factor(\n",
        "        self,\n",
        "        t: torch.Tensor, # time step, shape (n_batch, 1)\n",
        "    ) -> torch.Tensor:   # prefactor inside the MSE loss, shape (n_batch, 1)\n",
        "        return self.betas[t] / (np.sqrt(2) * self.sigmas[t] * self.sqrt_alphas[t] * self.sqrt_one_minus_alphas_bar[t])\n",
        "\n",
        "    def compute_mu_tilde_t(\n",
        "        self,\n",
        "        xt: torch.Tensor,    # noisy point x_t, shape (n_batch, data_dim)\n",
        "        t: torch.Tensor,     # time step, shape (n_batch, 1)\n",
        "        noise: torch.Tensor, # gaussian noise, shape (n_batch, data_dim)\n",
        "    ) -> torch.Tensor:       # computes mu_tilde_t, shape (n_batch, data_dim)\n",
        "        return (xt - noise * self.betas[t] / self.sqrt_one_minus_alphas_bar[t]) / self.sqrt_alphas[t]\n",
        "\n",
        "    def batch_loss(\n",
        "        self,\n",
        "        x: torch.Tensor, # input data, shape (n_batch, data_dim)\n",
        "    ) -> torch.Tensor:   # loss, shape (n_batch, )\n",
        "        # TODO: Compute the batch loss.\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        n_samples: int,        # number of samples\n",
        "        keep_xt: bool = False, # whether to keep the intermediate x_t\n",
        "    ) -> torch.Tensor:         # sampled data, shape (n_samples, data_dim) or (n_steps, n_samples, data_dim)\n",
        "        x = torch.randn(n_samples, self.data_dim)\n",
        "        if keep_xt:\n",
        "            xts = torch.zeros((n_samples, self.n_steps + 1, self.data_dim))\n",
        "            xts[:, self.n_steps] = x\n",
        "        for t in reversed(range(self.n_steps)):\n",
        "            z = torch.randn(n_samples, self.data_dim) if t > 0 else 0.\n",
        "            model_pred = self.net(torch.cat((x, torch.full((x.shape[0], 1), t, dtype=torch.float32)), dim=1))\n",
        "            x = self.compute_mu_tilde_t(x, t, model_pred) + self.sigmas[t] * z\n",
        "            if keep_xt:\n",
        "                xts[:, t] = x\n",
        "        return xts if keep_xt else x"
      ],
      "id": "dRRPVnih9Drv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzXygSv59Drv"
      },
      "source": [
        "### Training the model"
      ],
      "id": "dzXygSv59Drv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49ZdHicU9Drv"
      },
      "outputs": [],
      "source": [
        "ddpm = DDPM(\n",
        "    data_dim = 4,\n",
        "    n_steps = 100,\n",
        "    n_layers = 3,\n",
        "    hidden_dim = 64,\n",
        ")\n",
        "epochs = 50\n",
        "\n",
        "optimizer = torch.optim.Adam(ddpm.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    epoch_losses = []\n",
        "    for batch, x in enumerate(train_dataloader):\n",
        "        loss = ddpm.batch_loss(x)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "    epoch_loss = np.mean(epoch_losses)\n",
        "    print(f\"Epoch {epoch+1}: loss = {epoch_loss}\")\n",
        "    losses[epoch] = epoch_loss"
      ],
      "id": "49ZdHicU9Drv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N38Odq-89Drv"
      },
      "outputs": [],
      "source": [
        "# TODO plot epochs vs losses"
      ],
      "id": "N38Odq-89Drv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG27svDz9Drv"
      },
      "source": [
        "### Study the results"
      ],
      "id": "SG27svDz9Drv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjL9FOn9Drw"
      },
      "source": [
        "Let's generate some samples, invert the preprocessing and compute the observables from above:"
      ],
      "id": "KLjL9FOn9Drw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2Mc6Zak9Drw"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    sample = ddpm.sample(10000).cpu().numpy()\n",
        "sample_pp = invert_preprocessing(sample * train_std + train_mean)\n",
        "gen_event_im, _, gen_pt, _ = get_obs(sample_pp)"
      ],
      "id": "k2Mc6Zak9Drw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM04TPe39Drw"
      },
      "source": [
        "We can now plot some observables and compare them to the truth distribution. We start with the energy of the first muon."
      ],
      "id": "CM04TPe39Drw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxJOm_jx9Drw"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(0,200,50)\n",
        "axs.hist(sample_pp[:,0], bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_data[:,0], bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$E_1$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "KxJOm_jx9Drw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DLCH9FL9Drw"
      },
      "source": [
        "That looks quite nice! Next, we look at the $p_T$ again."
      ],
      "id": "-DLCH9FL9Drw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLt_3Pz39Drw"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(0, 70, 50)\n",
        "axs.hist(gen_pt, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_muon1_pts, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$p_T$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "QLt_3Pz39Drw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5EIs20y9Drw"
      },
      "source": [
        "We can see that it is difficult for the network to learn the sharp edge near $m_Z$. Learning such features is a typical difficulty of generative networks. Finally, we can take a look at $m_{\\mu\\mu}$. This observable is challenging to learn because the network needs to extract the correlation between different features correctly."
      ],
      "id": "q5EIs20y9Drw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gz4_zR39Drw"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(60, 120, 50)\n",
        "axs.hist(gen_event_im, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_event_ims, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(\"$m_{\\mu\\mu}$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "8gz4_zR39Drw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "volwdfu19Drx"
      },
      "source": [
        "#### Other observables\n",
        "\n",
        "Make histograms of the $\\phi$ and $\\eta$ observables for the first muon."
      ],
      "id": "volwdfu19Drx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO-B71Bo9Drx"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(-np.pi, np.pi, 50)\n",
        "gen_phi = np.arctan2(sample_pp[:,2], sample_pp[:,1])\n",
        "test_phi = np.arctan2(test_data[:,2], test_data[:,1])\n",
        "axs.hist(gen_phi, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_phi, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(r\"$\\phi$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "SO-B71Bo9Drx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyXR5hgu9Drx"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots()\n",
        "\n",
        "bins = np.linspace(-4, 4, 50)\n",
        "gen_eta = np.arctanh(sample_pp[:,3] / np.sqrt(sample_pp[:,1]**2 + sample_pp[:,2]**2 + sample_pp[:,3]**2))\n",
        "test_eta = np.arctanh(test_data[:,3] / np.sqrt(test_data[:,1]**2 + test_data[:,2]**2 + test_data[:,3]**2))\n",
        "axs.hist(gen_eta, bins=bins, density=True, histtype=\"step\", label=\"generated\")\n",
        "axs.hist(test_eta, bins=bins, density=True, histtype=\"step\", label=\"truth\")\n",
        "\n",
        "axs.set_xlabel(r\"$\\eta$ (GeV)\")\n",
        "axs.set_ylabel(\"normalized\")\n",
        "axs.legend()\n",
        "plt.show()"
      ],
      "id": "OyXR5hgu9Drx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cze3LUBxt5nB"
      },
      "id": "Cze3LUBxt5nB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}